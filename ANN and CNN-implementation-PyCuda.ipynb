{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfJJfUsrHQlW",
        "outputId": "ea23b1f3-a0fc-4c66-e5cd-52a2ddf120d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.10/dist-packages (2024.1.2)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.10/dist-packages (from pycuda) (2024.1.15)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (4.3.6)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA GENERATION"
      ],
      "metadata": {
        "id": "2FI1aMcqWijp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np\n",
        "import pycuda.driver as drv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pycuda.gpuarray as gpuarray\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time"
      ],
      "metadata": {
        "id": "E--8xr8aIJmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_data = np.array([\n",
        "    [[0, 0, 1, 0, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 0, 1, 0]],\n",
        "\n",
        "    [[0, 0, 1, 0, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0],\n",
        "     [0, 0, 1, 0, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1]],\n",
        "\n",
        "    [[0, 1, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 1, 0],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 0, 1]],\n",
        "\n",
        "    [[0, 0, 1, 1, 0],\n",
        "     [0, 1, 0, 0, 1],\n",
        "     [0, 1, 1, 1, 1],\n",
        "     [0, 1, 0, 0, 1],\n",
        "     [0, 1, 0, 0, 1]],\n",
        "\n",
        "    [[0, 1, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 0],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 1, 0],\n",
        "     [1, 0, 0, 1, 0]],\n",
        "\n",
        "    [[0, 1, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 1, 0],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 0, 1, 0, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [0, 0, 0, 0, 0]]\n",
        "])"
      ],
      "metadata": {
        "id": "HiJMEuapSOG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " M_data = np.array([\n",
        "    [[1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0]],\n",
        "\n",
        "    [[0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 0, 1]],\n",
        "\n",
        "    [[0, 1, 0, 1, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 0, 1, 0]],\n",
        "\n",
        "    [[1, 0, 0, 0, 1],\n",
        "     [1, 1, 0, 1, 1],\n",
        "     [1, 0, 1, 0, 1],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 0, 0, 0, 1]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 0, 1, 0]],\n",
        "\n",
        "    [[0, 1, 0, 1, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 0, 1]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0]]\n",
        "])"
      ],
      "metadata": {
        "id": "h7s1G96lZ2Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_data = np.array([\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 1, 0],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 0]],\n",
        "\n",
        "    [[0, 1, 1, 1, 1],\n",
        "     [0, 1, 0, 0, 1],\n",
        "     [0, 1, 1, 1, 1],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 0, 0, 1]],\n",
        "\n",
        "    [[1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 1, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 1, 1, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 1, 0, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 0],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 1, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 1, 0, 0],\n",
        "     [0, 1, 0, 1, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 0],\n",
        "     [0, 0, 1, 0, 1]],\n",
        "\n",
        "    [[1, 1, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 1, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 1, 1, 1, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [0, 1, 1, 0, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 0, 0, 0, 0]]\n",
        "])"
      ],
      "metadata": {
        "id": "RD-P9dMObLiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_data = A_data.reshape(10,25)\n",
        "M_data = M_data.reshape(10,25)\n",
        "R_data = R_data.reshape(10,25)"
      ],
      "metadata": {
        "id": "puqGlVJpma-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "labels = []\n",
        "for k,i in enumerate([A_data,M_data,R_data]):\n",
        "    dataset.append(i)\n",
        "    for j in range(10):\n",
        "        labels.append(k)\n",
        "dataset = np.concatenate(dataset,axis = 0)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "kZKDCnclmkhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZWG7gi4nWRW",
        "outputId": "df8ba70b-18ab-4228-ede2-ef5471d48ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuHvzLy1nfDB",
        "outputId": "fe9bc45c-68a9-4b60-c541-fd9e6e67f363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30,)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(dataset,labels,train_size = 0.7)"
      ],
      "metadata": {
        "id": "FRMK4CtOnkuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 21\n",
        "input_size = 25\n",
        "hidden_size = 10\n",
        "output_size = 3\n",
        "learning_rate = 0.01\n",
        "\n",
        "w1 = np.random.randn(input_size, hidden_size).astype(np.float32)\n",
        "w2 = np.random.randn(hidden_size, output_size).astype(np.float32)\n",
        "\n",
        "w1_gpu = drv.mem_alloc(w1.nbytes)\n",
        "w2_gpu = drv.mem_alloc(w2.nbytes)\n",
        "drv.memcpy_htod(w1_gpu, w1)\n",
        "drv.memcpy_htod(w2_gpu, w2)"
      ],
      "metadata": {
        "id": "ryA8N-l9noOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANN(model)**"
      ],
      "metadata": {
        "id": "Or8RwQmGn46X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod= SourceModule(\"\"\"\n",
        "__global__ void forward(float *input, float *w1, float *w2, float *b1, float *b2,\n",
        "                       int input_size, int hidden_size, int output_size, int batch_size) {\n",
        "    int batch_idx    =    blockIdx.x;\n",
        "    int hidden_idx   =    threadIdx.x;\n",
        "\n",
        "    // Compute first layer activations\n",
        "    if (hidden_idx  <  hidden_size) {\n",
        "        float sum  =  0.0f;\n",
        "        for (int i = 0; i < input_size ; i++) {\n",
        "            sum  +=  input[batch_idx * input_size + i] *  w1[i * hidden_size + hidden_idx];\n",
        "        }\n",
        "        b1[batch_idx * hidden_size + hidden_idx] = fmaxf(0.0f, sum); // ReLU activation\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // Ensure all threads complete b1 computation before moving to b2\n",
        "\n",
        "    // Compute second layer activations\n",
        "    int output_idx  =  threadIdx.x;\n",
        "    if (output_idx  <  output_size) {\n",
        "        float sum   =  0.0f;\n",
        "        for (int  j = 0; j <  hidden_size;  j++) {\n",
        "            sum += b1[batch_idx * hidden_size + j] * w2[j * output_size + output_idx];\n",
        "        }\n",
        "        b2[batch_idx * output_size + output_idx] = sum; // Output layer without activation\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backprop(float *input, float *b1, float *b2, float *target, float *w1, float *w2,\n",
        "                        int input_size, int hidden_size, int output_size, int batch_size, float learning_rate) {\n",
        "    int batch_idx   =  blockIdx.x;\n",
        "    int output_idx  =  threadIdx.x;\n",
        "\n",
        "    // Calculate gradients and update weights for the second layer\n",
        "    if (output_idx < output_size) {\n",
        "        float delta2 = b2[batch_idx * output_size + output_idx] - target[batch_idx * output_size + output_idx];\n",
        "        for (int j = 0; j <  hidden_size  ; j++) {\n",
        "            atomicAdd(&w2[j * output_size + output_idx], -learning_rate * delta2 * b1[batch_idx * hidden_size + j]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    // Calculate gradients and update weights for the first layer\n",
        "    int hidden_idx = threadIdx.x;\n",
        "    if (hidden_idx < hidden_size) {\n",
        "        float delta1   =   0.0f;\n",
        "        for (int k = 0; k < output_size; k++) {\n",
        "            delta1 += (b2[batch_idx * output_size + k] - target[batch_idx * output_size + k])\n",
        "                     * w2[hidden_idx * output_size + k];\n",
        "        }\n",
        "        delta1 *=  (b1[batch_idx * hidden_size + hidden_idx] > 0.0f) ? 1.0f : 0.0f; // Derivative of ReLU\n",
        "\n",
        "        for (int i = 0; i  < input_size; i++) {\n",
        "            atomicAdd(&w1[i * hidden_size + hidden_idx], -learning_rate * delta1 * input[batch_idx * input_size + i]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "forward_kernel = mod.get_function(\"forward\")\n",
        "backprop_kernel = mod.get_function(\"backprop\")"
      ],
      "metadata": {
        "id": "RpV9HTpJoApG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import BININT1\n",
        "import numpy as np\n",
        "import pycuda.autoinit\n",
        "import pycuda.gpuarray as gpuarray\n",
        "from pycuda.compiler import SourceModule\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class ANN:\n",
        "    def __init__(self, input_size=25, hidden_size=64, output_size=3, learning_rate=0.01):\n",
        "        # Initialize network parameters\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights with Xavier initialization\n",
        "        self.w1 = np.random.randn(input_size, hidden_size).astype(np.float32) * np.sqrt(2.0 / input_size)\n",
        "        self.w2 = np.random.randn(hidden_size, output_size).astype(np.float32) * np.sqrt(2.0 / hidden_size)\n",
        "\n",
        "        # Retrieve CUDA kernel functions for forward pass and backpropagation\n",
        "        self.forward_kernel = mod.get_function(\"forward\")\n",
        "        self.backprop_kernel = mod.get_function(\"backprop\")\n",
        "\n",
        "        # Transfer weights to GPU\n",
        "        self.w1_gpu = gpuarray.to_gpu(self.w1)\n",
        "        self.w2_gpu = gpuarray.to_gpu(self.w2)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        # Compute softmax across the last axis (class dimension)\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def train_batch(self, batch_x, batch_y):\n",
        "        # Perform a single batch of training with forward and backward passes\n",
        "        batch_size = len(batch_x)\n",
        "\n",
        "        # Transfer input and target batch data to GPU\n",
        "        inputs_gpu = gpuarray.to_gpu(batch_x.astype(np.float32))\n",
        "        targets_gpu = gpuarray.to_gpu(batch_y.astype(np.float32))\n",
        "\n",
        "        # Initialize GPU arrays for hidden layer and output layer activations\n",
        "        b1_gpu = gpuarray.zeros((batch_size, self.hidden_size), dtype=np.float32)\n",
        "        b2_gpu = gpuarray.zeros((batch_size, self.output_size), dtype=np.float32)\n",
        "\n",
        "        # Set up the block and grid dimensions for kernel launches\n",
        "        block_dim = (max(self.hidden_size, self.output_size), 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        # Forward pass\n",
        "        self.forward_kernel(inputs_gpu, self.w1_gpu, self.w2_gpu, b1_gpu, b2_gpu,\n",
        "                            np.int32(self.input_size), np.int32(self.hidden_size),\n",
        "                            np.int32(self.output_size), np.int32(batch_size),\n",
        "                            block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Apply softmax activation to the output layer\n",
        "        outputs = self.softmax(b2_gpu.get())\n",
        "        b2_gpu.gpudata.free()\n",
        "        b2_gpu = gpuarray.to_gpu(outputs.astype(np.float32))\n",
        "\n",
        "        # Backward pass (gradient update)\n",
        "        self.backprop_kernel(inputs_gpu, b1_gpu, b2_gpu, targets_gpu,\n",
        "                             self.w1_gpu, self.w2_gpu,\n",
        "                             np.int32(self.input_size), np.int32(self.hidden_size),\n",
        "                             np.int32(self.output_size), np.int32(batch_size),\n",
        "                             np.float32(self.learning_rate),\n",
        "                             block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Free GPU memory\n",
        "        inputs_gpu.gpudata.free()\n",
        "        targets_gpu.gpudata.free()\n",
        "        b1_gpu.gpudata.free()\n",
        "        b2_gpu.gpudata.free()\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict class probabilities for a given batch of inputs\n",
        "        batch_size = len(X)\n",
        "\n",
        "        # Transfer input data to GPU\n",
        "        inputs_gpu = gpuarray.to_gpu(X.astype(np.float32))\n",
        "\n",
        "        # Initialize GPU arrays for activations\n",
        "        b1_gpu = gpuarray.zeros((batch_size, self.hidden_size), dtype=np.float32)\n",
        "        b2_gpu = gpuarray.zeros((batch_size, self.output_size), dtype=np.float32)\n",
        "\n",
        "        # Kernel launch dimensions\n",
        "        block_dim = (max(self.hidden_size, self.output_size), 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        # Forward pass\n",
        "        self.forward_kernel(inputs_gpu, self.w1_gpu, self.w2_gpu, b1_gpu, b2_gpu,\n",
        "                            np.int32(self.input_size), np.int32(self.hidden_size),\n",
        "                            np.int32(self.output_size), np.int32(batch_size),\n",
        "                            block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Get output predictions and apply softmax\n",
        "        outputs = self.softmax(b2_gpu.get())\n",
        "\n",
        "        # Free GPU memory\n",
        "        inputs_gpu.gpudata.free()\n",
        "        b1_gpu.gpudata.free()\n",
        "        b2_gpu.gpudata.free()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, epochs=100, batch_size=32):\n",
        "    # Initialize the neural network with specified parameters\n",
        "    net = ANN(input_size=25, hidden_size=10, output_size=3)\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_train_onehot = np.zeros((len(y_train), 3))\n",
        "    y_train_onehot[np.arange(len(y_train)), y_train] = 1\n",
        "\n",
        "    n_batches = len(X_train) // batch_size\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle training data for each epoch\n",
        "        indices = np.random.permutation(len(X_train))\n",
        "        X_train_shuffled = X_train[indices]\n",
        "        y_train_shuffled = y_train_onehot[indices]\n",
        "\n",
        "        # Batch training\n",
        "        for batch in range(n_batches):\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "            batch_x = X_train_shuffled[start_idx:end_idx]\n",
        "            batch_y = y_train_shuffled[start_idx:end_idx]\n",
        "            net.train_batch(batch_x, batch_y)\n",
        "\n",
        "        # Periodic evaluation on training data\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            train_preds = np.argmax(net.predict(X_train), axis=1)\n",
        "            train_acc = accuracy_score(y_train, train_preds)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Training Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    # Evaluation on test data\n",
        "    test_preds = np.argmax(net.predict(X_test), axis=1)\n",
        "    test_acc = accuracy_score(y_test, test_preds)\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    return net\n"
      ],
      "metadata": {
        "id": "4C_hl_NMsFg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_and_evaluate(X_train, y_train, X_test, y_test, epochs=50, batch_size=21)"
      ],
      "metadata": {
        "id": "cdHWYwqsukWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test accuracy:0.4444"
      ],
      "metadata": {
        "id": "MrZJYZAoJpqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "5_OJKknJxi4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "/*\n",
        "  Convolutional Layer Forward Pass: Applies 2D convolution with ReLU activation\n",
        "*/\n",
        "__global__ void conv2d(float *input, float *filters, float *output,\n",
        "                      int batch_size, int height, int width, int channels,\n",
        "                      int num_filters, int kernel_size, int output_height, int output_width) {\n",
        "    int batch_idx = blockIdx.x; // batch index\n",
        "    int filter_idx = blockIdx.y; // filter index\n",
        "    int out_y = threadIdx.x;     // output y-coordinate\n",
        "    int out_x = threadIdx.y;     // output x-coordinate\n",
        "\n",
        "    if (out_y < output_height && out_x < output_width) {\n",
        "        float sum = 0.0f; // accumulator for convolution sum\n",
        "\n",
        "        // Loop over each channel and kernel position\n",
        "        for (int c = 0; c < channels; c++) {\n",
        "            for (int ky = 0; ky < kernel_size; ky++) {\n",
        "                for (int kx = 0; kx < kernel_size; kx++) {\n",
        "                    int in_y = out_y + ky; // input y-coordinate\n",
        "                    int in_x = out_x + kx; // input x-coordinate\n",
        "\n",
        "                    if (in_y < height && in_x < width) {\n",
        "                        int input_idx = batch_idx * (height * width * channels) +\n",
        "                                        c * (height * width) +\n",
        "                                        in_y * width + in_x;\n",
        "                        int filter_idx_full = filter_idx * (channels * kernel_size * kernel_size) +\n",
        "                                              c * (kernel_size * kernel_size) +\n",
        "                                              ky * kernel_size + kx;\n",
        "\n",
        "                        sum += input[input_idx] * filters[filter_idx_full];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        int output_idx = batch_idx * (num_filters * output_height * output_width) +\n",
        "                         filter_idx * (output_height * output_width) +\n",
        "                         out_y * output_width + out_x;\n",
        "        output[output_idx] = fmaxf(0.0f, sum); // ReLU activation\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  Max Pooling Layer Forward Pass: Applies max pooling operation\n",
        "*/\n",
        "__global__ void max_pool2d(float *input, float *output,\n",
        "                          int batch_size, int height, int width, int channels,\n",
        "                          int pool_size, int output_height, int output_width) {\n",
        "    int batch_idx = blockIdx.x; // batch index\n",
        "    int channel = blockIdx.y;   // channel index\n",
        "    int out_y = threadIdx.x;    // output y-coordinate\n",
        "    int out_x = threadIdx.y;    // output x-coordinate\n",
        "\n",
        "    if (out_y < output_height && out_x < output_width) {\n",
        "        float max_val = -1e10f; // initialize to very low value\n",
        "\n",
        "        // Loop over each position within the pooling window\n",
        "        for (int py = 0; py < pool_size; py++) {\n",
        "            for (int px = 0; px < pool_size; px++) {\n",
        "                int in_y = out_y * pool_size + py;\n",
        "                int in_x = out_x * pool_size + px;\n",
        "\n",
        "                if (in_y < height && in_x < width) {\n",
        "                    int input_idx = batch_idx * (channels * height * width) +\n",
        "                                    channel * (height * width) +\n",
        "                                    in_y * width + in_x;\n",
        "                    max_val = fmaxf(max_val, input[input_idx]);\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        int output_idx = batch_idx * (channels * output_height * output_width) +\n",
        "                         channel * (output_height * output_width) +\n",
        "                         out_y * output_width + out_x;\n",
        "        output[output_idx] = max_val;\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  Fully Connected Layer Forward Pass\n",
        "*/\n",
        "__global__ void fc_forward(float *input, float *weights, float *output,\n",
        "                          int batch_size, int input_size, int output_size) {\n",
        "    int batch_idx = blockIdx.x;     // batch index\n",
        "    int neuron_idx = threadIdx.x;   // neuron index in the fully connected layer\n",
        "\n",
        "    if (neuron_idx < output_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < input_size; i++) {\n",
        "            sum += input[batch_idx * input_size + i] * weights[i * output_size + neuron_idx];\n",
        "        }\n",
        "        output[batch_idx * output_size + neuron_idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  Convolutional Layer Backward Pass\n",
        "*/\n",
        "__global__ void conv2d_backward(float *d_output, float *filters, float *d_input,\n",
        "                               int batch_size, int height, int width, int channels,\n",
        "                               int num_filters, int kernel_size, int output_height, int output_width) {\n",
        "    int batch_idx = blockIdx.x; // batch index\n",
        "    int channel = blockIdx.y;   // channel index\n",
        "    int in_y = threadIdx.x;     // input y-coordinate\n",
        "    int in_x = threadIdx.y;     // input x-coordinate\n",
        "\n",
        "    if (in_y < height && in_x < width) {\n",
        "        float sum = 0.0f;\n",
        "\n",
        "        for (int f = 0; f < num_filters; f++) {\n",
        "            for (int ky = 0; ky < kernel_size; ky++) {\n",
        "                for (int kx = 0; kx < kernel_size; kx++) {\n",
        "                    int out_y = in_y - ky;\n",
        "                    int out_x = in_x - kx;\n",
        "\n",
        "                    if (out_y >= 0 && out_y < output_height && out_x >= 0 && out_x < output_width) {\n",
        "                        int d_output_idx = batch_idx * (num_filters * output_height * output_width) +\n",
        "                                           f * (output_height * output_width) +\n",
        "                                           out_y * output_width + out_x;\n",
        "                        int filter_idx = f * (channels * kernel_size * kernel_size) +\n",
        "                                         channel * (kernel_size * kernel_size) +\n",
        "                                         ky * kernel_size + kx;\n",
        "\n",
        "                        sum += d_output[d_output_idx] * filters[filter_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        int d_input_idx = batch_idx * (channels * height * width) +\n",
        "                          channel * (height * width) +\n",
        "                          in_y * width + in_x;\n",
        "        d_input[d_input_idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  Convolutional Layer Weights Update\n",
        "*/\n",
        "__global__ void conv2d_update_weights(float *input, float *d_output, float *d_weights,\n",
        "                                    int batch_size, int height, int width, int channels,\n",
        "                                    int num_filters, int kernel_size, int output_height, int output_width) {\n",
        "    int filter_idx = blockIdx.x;    // filter index\n",
        "    int ky = threadIdx.x;           // kernel y-coordinate\n",
        "    int kx = threadIdx.y;           // kernel x-coordinate\n",
        "    int c = blockIdx.y;             // channel index\n",
        "\n",
        "    if (ky < kernel_size && kx < kernel_size) {\n",
        "        float sum = 0.0f;\n",
        "\n",
        "        for (int b = 0; b < batch_size; b++) {\n",
        "            for (int out_y = 0; out_y < output_height; out_y++) {\n",
        "                for (int out_x = 0; out_x < output_width; out_x++) {\n",
        "                    int in_y = out_y + ky;\n",
        "                    int in_x = out_x + kx;\n",
        "\n",
        "                    if (in_y < height && in_x < width) {\n",
        "                        int input_idx = b * (channels * height * width) +\n",
        "                                        c * (height * width) +\n",
        "                                        in_y * width + in_x;\n",
        "                        int d_output_idx = b * (num_filters * output_height * output_width) +\n",
        "                                           filter_idx * (output_height * output_width) +\n",
        "                                           out_y * output_width + out_x;\n",
        "\n",
        "                        sum += input[input_idx] * d_output[d_output_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        int weight_idx = filter_idx * (channels * kernel_size * kernel_size) +\n",
        "                         c * (kernel_size * kernel_size) +\n",
        "                         ky * kernel_size + kx;\n",
        "        d_weights[weight_idx] = sum / batch_size;\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  Max Pooling Layer Backward Pass\n",
        "*/\n",
        "__global__ void max_pool_backward(float *input, float *d_output, float *d_input,\n",
        "                                int batch_size, int height, int width, int channels,\n",
        "                                int pool_size, int output_height, int output_width) {\n",
        "    int batch_idx = blockIdx.x;     // batch index\n",
        "    int channel = blockIdx.y;       // channel index\n",
        "    int in_y = threadIdx.x;         // input y-coordinate\n",
        "    int in_x = threadIdx.y;         // input x-coordinate\n",
        "\n",
        "    if (in_y < height && in_x < width) {\n",
        "        int out_y = in_y / pool_size;\n",
        "        int out_x = in_x / pool_size;\n",
        "\n",
        "        if (out_y < output_height && out_x < output_width) {\n",
        "            int input_idx = batch_idx * (channels * height * width) +\n",
        "                            channel * (height * width) +\n",
        "                            in_y * width + in_x;\n",
        "            int output_idx = batch_idx * (channels * output_height * output_width) +\n",
        "                             channel * (output_height * output_width) +\n",
        "                             out_y * output_width + out_x;\n",
        "            float input_val = input[input_idx];\n",
        "            float max_val = -1e10f;\n",
        "            int max_idx_y = -1, max_idx_x = -1;\n",
        "\n",
        "            for (int py = 0; py < pool_size; py++) {\n",
        "                for (int px = 0; px < pool_size; px++) {\n",
        "                    int curr_y = out_y * pool_size + py;\n",
        "                    int curr_x = out_x * pool_size + px;\n",
        "\n",
        "                    if (curr_y < height && curr_x < width) {\n",
        "                        int curr_idx = batch_idx * (channels * height * width) +\n",
        "                                       channel * (height * width) +\n",
        "                                       curr_y * width + curr_x;\n",
        "                        float curr_val = input[curr_idx];\n",
        "\n",
        "                        if (curr_val > max_val) {\n",
        "                            max_val = curr_val;\n",
        "                            max_idx_y = curr_y;\n",
        "                            max_idx_x = curr_x;\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            if (in_y == max_idx_y && in_x == max_idx_x) {\n",
        "                d_input[input_idx] = d_output[output_idx];\n",
        "            } else {\n",
        "                d_input[input_idx] = 0.0f;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  Fully Connected Layer Backward Pass\n",
        "*/\n",
        "__global__ void fc_backward(float *d_output, float *weights, float *d_input,\n",
        "                           int batch_size, int input_size, int output_size) {\n",
        "    int batch_idx = blockIdx.x;    // batch index\n",
        "    int input_idx = threadIdx.x;   // input index for current neuron\n",
        "\n",
        "    if (input_idx < input_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int j = 0; j < output_size; j++) {\n",
        "            sum += d_output[batch_idx * output_size + j] * weights[input_idx * output_size + j];\n",
        "        }\n",
        "        d_input[batch_idx * input_size + input_idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  Fully Connected Layer Weight Update\n",
        "*/\n",
        "__global__ void fc_update_weights(float *input, float *d_output, float *d_weights,\n",
        "                                int batch_size, int input_size, int output_size) {\n",
        "    int in_idx = blockIdx.x;    // input index\n",
        "    int out_idx = threadIdx.x;  // output index\n",
        "\n",
        "    if (in_idx < input_size && out_idx < output_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int b = 0; b < batch_size; b++) {\n",
        "            sum += input[b * input_size + in_idx] * d_output[b * output_size + out_idx];\n",
        "        }\n",
        "        d_weights[in_idx * output_size + out_idx] = sum / batch_size;\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  ReLU Activation Function: Applies ReLU on input array\n",
        "*/\n",
        "__global__ void relu(float *x, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < n) {\n",
        "        x[idx] = max(0.0, x[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        "  Softmax Activation Function: Applies softmax on input array\n",
        "*/\n",
        "__global__ void softmax(float *x, int batch_size, int num_classes) {\n",
        "    int batch_idx = blockIdx.x;  // batch index\n",
        "    int class_idx = threadIdx.x; // class index\n",
        "\n",
        "    int idx = batch_idx * num_classes + class_idx;\n",
        "\n",
        "    float max_val = -1e9;\n",
        "    for (int j = 0; j < num_classes; j++) {\n",
        "        max_val = fmaxf(max_val, x[batch_idx * num_classes + j]);\n",
        "    }\n",
        "\n",
        "    float sum_exp = 0.0;\n",
        "    for (int j = 0; j < num_classes; j++) {\n",
        "        int current_idx = batch_idx * num_classes + j;\n",
        "        x[current_idx] = expf(x[current_idx] - max_val);\n",
        "        sum_exp += x[current_idx];\n",
        "    }\n",
        "\n",
        "    x[idx] = x[idx] / sum_exp;\n",
        "}\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "2XYdDtyyxrjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.conv1_filters = 32\n",
        "        self.conv1_size = 3\n",
        "        self.pool_size = 2\n",
        "        self.fc1_size = 10\n",
        "        self.num_classes = 3\n",
        "\n",
        "        self.input_size = 5\n",
        "        self.conv1_output_size = self.input_size - self.conv1_size + 1\n",
        "        self.pool1_output_size = self.conv1_output_size // self.pool_size\n",
        "\n",
        "        conv1_weights = np.random.randn(self.conv1_filters, 1,\n",
        "                                      self.conv1_size, self.conv1_size).astype(np.float32) * 0.1\n",
        "        self.conv1_weights_gpu = gpuarray.to_gpu(conv1_weights)\n",
        "\n",
        "        fc1_input_size = self.conv1_filters * self.pool1_output_size * self.pool1_output_size\n",
        "        fc1_weights = np.random.randn(fc1_input_size, self.fc1_size).astype(np.float32) * 0.1\n",
        "        self.fc1_weights_gpu = gpuarray.to_gpu(fc1_weights)\n",
        "\n",
        "        fc2_weights = np.random.randn(self.fc1_size, self.num_classes).astype(np.float32) * 0.1\n",
        "        self.fc2_weights_gpu = gpuarray.to_gpu(fc2_weights)\n",
        "\n",
        "        self.max_pool = mod.get_function(\"max_pool2d\")\n",
        "        self.fc_forward = mod.get_function(\"fc_forward\")\n",
        "        self.conv_forward = mod.get_function(\"conv2d\")\n",
        "        self.conv_backward = mod.get_function(\"conv2d_backward\")\n",
        "        self.conv_update_weights = mod.get_function(\"conv2d_update_weights\")\n",
        "        self.pool_backward = mod.get_function(\"max_pool_backward\")\n",
        "        self.fc_backward = mod.get_function(\"fc_backward\")\n",
        "        self.fc_update_weights = mod.get_function(\"fc_update_weights\")\n",
        "        self.relu = mod.get_function(\"relu\")\n",
        "        self.softmax = mod.get_function(\"softmax\")\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.eval_accuracies = []\n",
        "\n",
        "    def backward(self, x_batch, y_batch, output):\n",
        "        batch_size = len(x_batch)\n",
        "\n",
        "        d_output = output.copy()\n",
        "        d_output[range(batch_size), y_batch] -= 1\n",
        "        d_output /= batch_size\n",
        "\n",
        "        d_fc2_output_gpu = gpuarray.to_gpu(d_output)\n",
        "        d_fc1_output = gpuarray.zeros((batch_size, self.fc1_size), dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.fc1_size, 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.fc_backward(d_fc2_output_gpu, self.fc2_weights_gpu, d_fc1_output,\n",
        "                        np.int32(batch_size), np.int32(self.fc1_size), np.int32(self.num_classes),\n",
        "                        block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_fc2_weights = gpuarray.zeros_like(self.fc2_weights_gpu)\n",
        "\n",
        "        block_dim = (self.num_classes, 1, 1)\n",
        "        grid_dim = (self.fc1_size, 1, 1)\n",
        "\n",
        "        self.fc_update_weights(d_fc1_output, d_fc2_output_gpu, d_fc2_weights,\n",
        "                             np.int32(batch_size), np.int32(self.fc1_size), np.int32(self.num_classes),\n",
        "                             block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_pool1_output = gpuarray.zeros((batch_size, self.conv1_filters,\n",
        "                                       self.pool1_output_size, self.pool1_output_size),\n",
        "                                      dtype=np.float32)\n",
        "\n",
        "        flattened_size = self.conv1_filters * self.pool1_output_size * self.pool1_output_size\n",
        "\n",
        "        block_dim = (flattened_size, 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.fc_backward(d_fc1_output, self.fc1_weights_gpu, d_pool1_output,\n",
        "                        np.int32(batch_size), np.int32(flattened_size), np.int32(self.fc1_size),\n",
        "                        block=block_dim, grid=grid_dim)\n",
        "        d_fc1_weights = gpuarray.zeros_like(self.fc1_weights_gpu)\n",
        "\n",
        "        block_dim = (self.fc1_size, 1, 1)\n",
        "        grid_dim = (flattened_size, 1, 1)\n",
        "\n",
        "        self.fc_update_weights(d_pool1_output, d_fc1_output, d_fc1_weights,\n",
        "                             np.int32(batch_size), np.int32(flattened_size), np.int32(self.fc1_size),\n",
        "                             block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_conv1_output = gpuarray.zeros((batch_size, self.conv1_filters,\n",
        "                                       self.conv1_output_size, self.conv1_output_size),\n",
        "                                      dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.conv1_output_size, self.conv1_output_size, 1)\n",
        "        grid_dim = (batch_size, self.conv1_filters, 1)\n",
        "\n",
        "        self.pool_backward(d_conv1_output, d_pool1_output, d_conv1_output,\n",
        "                          np.int32(batch_size), np.int32(self.conv1_output_size),\n",
        "                          np.int32(self.conv1_output_size), np.int32(self.conv1_filters),\n",
        "                          np.int32(self.pool_size), np.int32(self.pool1_output_size),\n",
        "                          np.int32(self.pool1_output_size),\n",
        "                          block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_input = gpuarray.zeros((batch_size, 1, 5, 5), dtype=np.float32)\n",
        "\n",
        "        block_dim = (5, 5, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.conv_backward(d_conv1_output, self.conv1_weights_gpu, d_input,\n",
        "                          np.int32(batch_size), np.int32(5), np.int32(5), np.int32(1),\n",
        "                          np.int32(self.conv1_filters), np.int32(self.conv1_size),\n",
        "                          np.int32(self.conv1_output_size), np.int32(self.conv1_output_size),\n",
        "                          block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_conv1_weights = gpuarray.zeros_like(self.conv1_weights_gpu)\n",
        "\n",
        "        block_dim = (self.conv1_size, self.conv1_size, 1)\n",
        "        grid_dim = (self.conv1_filters, 1, 1)\n",
        "\n",
        "        self.conv_update_weights(d_input, d_conv1_output, d_conv1_weights,\n",
        "                               np.int32(batch_size), np.int32(5), np.int32(5), np.int32(1),\n",
        "                               np.int32(self.conv1_filters), np.int32(self.conv1_size),\n",
        "                               np.int32(self.conv1_output_size), np.int32(self.conv1_output_size),\n",
        "                               block=block_dim, grid=grid_dim)\n",
        "\n",
        "        self.conv1_weights_gpu -= self.learning_rate * d_conv1_weights\n",
        "        self.fc1_weights_gpu -= self.learning_rate * d_fc1_weights\n",
        "        self.fc2_weights_gpu -= self.learning_rate * d_fc2_weights\n",
        "\n",
        "        d_fc2_output_gpu.gpudata.free()\n",
        "        d_fc1_output.gpudata.free()\n",
        "        d_pool1_output.gpudata.free()\n",
        "        d_conv1_output.gpudata.free()\n",
        "        d_input.gpudata.free()\n",
        "        d_conv1_weights.gpudata.free()\n",
        "        d_fc1_weights.gpudata.free()\n",
        "        d_fc2_weights.gpudata.free()\n",
        "\n",
        "\n",
        "    def compute_loss(self, output, y_batch):\n",
        "        \"\"\"Compute categorical cross-entropy loss.\"\"\"\n",
        "        batch_size = len(y_batch)\n",
        "        output_cpu = output.get()\n",
        "        log_likelihood = -np.log(output_cpu[range(batch_size), y_batch])\n",
        "        loss = np.sum(log_likelihood) / batch_size\n",
        "        return loss\n",
        "\n",
        "    def compute_accuracy(self, output, y_batch):\n",
        "        \"\"\"Compute classification accuracy.\"\"\"\n",
        "        predictions = np.argmax(output.get(), axis=1)\n",
        "        return np.mean(predictions == y_batch)\n",
        "\n",
        "    def forward(self, x_batch):\n",
        "        \"\"\"Forward pass returning output probabilities.\"\"\"\n",
        "        batch_size = len(x_batch)\n",
        "\n",
        "        input_gpu = gpuarray.to_gpu(x_batch.astype(np.float32))\n",
        "\n",
        "        # Convolution layer\n",
        "        conv1_output = gpuarray.zeros((batch_size, self.conv1_filters,\n",
        "                                     self.conv1_output_size, self.conv1_output_size),\n",
        "                                    dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.conv1_output_size, self.conv1_output_size, 1)\n",
        "        grid_dim = (batch_size, self.conv1_filters, 1)\n",
        "\n",
        "        self.conv_forward(input_gpu, self.conv1_weights_gpu, conv1_output,\n",
        "                         np.int32(batch_size), np.int32(5), np.int32(5), np.int32(1),\n",
        "                         np.int32(self.conv1_filters), np.int32(self.conv1_size),\n",
        "                         np.int32(self.conv1_output_size), np.int32(self.conv1_output_size),\n",
        "                         block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Max pooling layer\n",
        "        pool1_output = gpuarray.zeros((batch_size, self.conv1_filters,\n",
        "                                     self.pool1_output_size, self.pool1_output_size),\n",
        "                                    dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.pool1_output_size, self.pool1_output_size, 1)\n",
        "        grid_dim = (batch_size, self.conv1_filters, 1)\n",
        "\n",
        "        self.max_pool(conv1_output, pool1_output,\n",
        "                     np.int32(batch_size), np.int32(self.conv1_output_size),\n",
        "                     np.int32(self.conv1_output_size), np.int32(self.conv1_filters),\n",
        "                     np.int32(self.pool_size), np.int32(self.pool1_output_size),\n",
        "                     np.int32(self.pool1_output_size),\n",
        "                     block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Flatten pooling output\n",
        "        flattened = pool1_output.reshape(batch_size, -1)\n",
        "\n",
        "        # First fully connected layer\n",
        "        fc1_output = gpuarray.zeros((batch_size, self.fc1_size), dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.fc1_size, 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.fc_forward(flattened, self.fc1_weights_gpu, fc1_output,\n",
        "                       np.int32(batch_size), np.int32(flattened.shape[1]),\n",
        "                       np.int32(self.fc1_size),\n",
        "                       block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # ReLU activation\n",
        "        self.relu(fc1_output, np.int32(fc1_output.size),\n",
        "                 block=(256, 1, 1), grid=((fc1_output.size + 255) // 256, 1, 1))\n",
        "\n",
        "        # Second fully connected layer (output layer)\n",
        "        output = gpuarray.zeros((batch_size, self.num_classes), dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.num_classes, 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.fc_forward(fc1_output, self.fc2_weights_gpu, output,\n",
        "                       np.int32(batch_size), np.int32(self.fc1_size),\n",
        "                       np.int32(self.num_classes),\n",
        "                       block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Softmax activation\n",
        "        self.softmax(output, np.int32(batch_size), np.int32(self.num_classes),\n",
        "                    block=(self.num_classes, 1, 1), grid=(batch_size, 1, 1))\n",
        "\n",
        "        return output, conv1_output, pool1_output, fc1_output\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=10, batch_size=32):\n",
        "        \"\"\"Train the CNN using mini-batch gradient descent.\"\"\"\n",
        "        n_samples = len(X_train)\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            epoch_accuracy = 0\n",
        "\n",
        "            for batch in range(n_batches):\n",
        "                start_idx = batch * batch_size\n",
        "                end_idx = min((batch + 1) * batch_size, n_samples)\n",
        "\n",
        "                x_batch = X_train[start_idx:end_idx]\n",
        "                y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "                output, conv1_output, pool1_output, fc1_output = self.forward(x_batch)\n",
        "\n",
        "                batch_loss = self.compute_loss(output, y_batch)\n",
        "                batch_accuracy = self.compute_accuracy(output, y_batch)\n",
        "\n",
        "                epoch_loss += batch_loss * len(x_batch)\n",
        "                epoch_accuracy += batch_accuracy * len(x_batch)\n",
        "\n",
        "                self.backward(x_batch, y_batch, output.get())\n",
        "\n",
        "                conv1_output.gpudata.free()\n",
        "                pool1_output.gpudata.free()\n",
        "                fc1_output.gpudata.free()\n",
        "                output.gpudata.free()\n",
        "\n",
        "            epoch_loss /= n_samples\n",
        "            epoch_accuracy /= n_samples\n",
        "\n",
        "            self.train_losses.append(epoch_loss)\n",
        "            self.train_accuracies.append(epoch_accuracy)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    def evaluate(self, X_test, y_test, batch_size=32):\n",
        "        \"\"\"Evaluate the CNN on test data.\"\"\"\n",
        "        n_samples = len(X_test)\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "        total_accuracy = 0\n",
        "\n",
        "        for batch in range(n_batches):\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = min((batch + 1) * batch_size, n_samples)\n",
        "\n",
        "            x_batch = X_test[start_idx:end_idx]\n",
        "            y_batch = y_test[start_idx:end_idx]\n",
        "\n",
        "            output, conv1_output, pool1_output, fc1_output = self.forward(x_batch)\n",
        "\n",
        "            batch_accuracy = self.compute_accuracy(output, y_batch)\n",
        "            total_accuracy += batch_accuracy * len(x_batch)\n",
        "\n",
        "            conv1_output.gpudata.free()\n",
        "            pool1_output.gpudata.free()\n",
        "            fc1_output.gpudata.free()\n",
        "            output.gpudata.free()\n",
        "\n",
        "        return total_accuracy / n_samples\n",
        "\n",
        "    def predict(self, X, batch_size=32):\n",
        "        \"\"\"Generate predictions for input data.\"\"\"\n",
        "        n_samples = len(X)\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "        predictions = np.zeros(n_samples, dtype=np.int32)\n",
        "\n",
        "        for batch in range(n_batches):\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = min((batch + 1) * batch_size, n_samples)\n",
        "\n",
        "            x_batch = X[start_idx:end_idx]\n",
        "\n",
        "            output, conv1_output, pool1_output, fc1_output = self.forward(x_batch)\n",
        "\n",
        "            batch_predictions = np.argmax(output.get(), axis=1)\n",
        "            predictions[start_idx:end_idx] = batch_predictions\n",
        "\n",
        "            # Free GPU memory\n",
        "            conv1_output.gpudata.free()\n",
        "            pool1_output.gpudata.free()\n",
        "            fc1_output.gpudata.free()\n",
        "            output.gpudata.free()\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "G_LqX3h0xnFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN(learning_rate=1e-4)\n",
        "\n",
        "cnn.train(X_train, y_train, X_test, y_test, epochs=10, batch_size=21)\n",
        "test_accuracy = cnn.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy :- {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXQl-DFGxife",
        "outputId": "0f54ded9-2701-4ef5-adb2-94fe106c5f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 2/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 3/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 4/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 5/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 6/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 7/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 8/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 9/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Epoch 10/10 - Loss: 1.0986 - Accuracy: 0.1905\n",
            "Test Accuracy :- 0.4444444444444444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        }
      ]
    }
  ]
}